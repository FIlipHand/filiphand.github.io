---
title: GPU programming learning resources
date: '2025-10-02'
tags: ['cuda', 'gpu', 'learning', 'triton', 'mojo']
draft: false
summary: Place to store resources that I used, use or will use on my journey to learn GPU programming (and bunch of other stuff LLM related).
---

#### Books:
- "Programming Massively Parallel Processors: A Hands-on Approach" by Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj
- "AI Systems Performance Engineering: Optimizing Model Training and Inference Workloads with GPUs, CUDA, and PyTorch" by  Chris Fregly (*coming soon!*)

#### YouTube Channels:
- [GPU MODE](https://www.youtube.com/@GPUMODE)
- [0mean1sigma](https://www.youtube.com/@0mean1sigma)
- [Simon Oz](https://www.youtube.com/@szymonozog7862)

#### YouTube videos:
- [CUDA Programming Course - High-Performance Computing with GPUs](https://www.youtube.com/watch?v=86FAWCzIe_4)
- [CUDA + ThunderKittens, but increasingly drunk.](https://www.youtube.com/watch?v=xcpEl0cGCC4)
- [Zen, CUDA and Tensor Cores by Casey Muratori](https://www.youtube.com/watch?v=uBtuMsAY7J8)

#### Github repositories:
- ["gpt-fast" by PyTorch team](https://github.com/meta-pytorch/gpt-fast)
- [llm.c by karpathy](https://github.com/karpathy/llm.c)
- [nanochat by karpathy](https://github.com/karpathy/nanochat)
- [nano-vllm by deepseek engineer](https://github.com/GeeeekExplorer/nano-vllm)

#### Blog posts:
- [GPU Glossary by Modal](https://modal.com/gpu-glossary)
- [Roadmap: Understanding GPU Architecture by Cornell University](https://cvw.cac.cornell.edu/gpu-architecture)
- ["Accelerating Generative AI with PyTorch: Segment Anything, Fast" by PyTorch](https://pytorch.org/blog/accelerating-generative-ai/)
- ["Tiny-TPU"](https://www.tinytpu.com/)
- [Fast LLM Inference From Scratch](https://andrewkchan.dev/posts/yalm.html)
- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview)
- [How to Think About GPUs](https://jax-ml.github.io/scaling-book/gpus)
- [LLM Inference Economics from First Principles](https://www.tensoreconomics.com/p/llm-inference-economics-from-first)
- [Inside vLLM: Anatomy of a High-Throughput LLM Inference System](https://www.aleksagordic.com/blog/vllm)
- [Nvidia's H100: Funny L2, and Tons of Bandwidth](https://chipsandcheese.com/p/nvidias-h100-funny-l2-and-tons-of-bandwidth)
- [Stephen Diehl Blog](https://www.stephendiehl.com/tags/mlir/)
- [HuggingFace - "The Smol Training Playbook"](https://huggingface.com/spaces/HuggingFaceTB/smol-training-playbook#introduction)
- [NVIDIA Tensor Core Evolution: From Volta To Blackwell](https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell)
- [Implementing a fast Tensor Core matmul on the Ada Architecture](https://www.spatters.ca/mma-matmul)
- [Floating Point Visually Explained](https://fabiensanglard.net/floating_point_visually_explained/)
- [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)
- [Understanding the CUDA Compiler & PTX with a Top-K Kernel](https://blog.alpindale.net/posts/top_k_cuda/)

#### Other:
- [hazyresearch at Stanford](https://hazyresearch.stanford.edu/)
- [LocalLLama Reddit community](https://reddit.com/r/LocalLLaMA/)
- [Fixing ALL CUDA installation errors](https://www.rightnowai.co/blog/fixing-cuda-installation-issues)
- [torch.compile, the missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)

#### Exercises:
- [TensorTonic](https://www.tensortonic.com/)
- [LeetGPU](https://leetgpu.com/)
- [MojoðŸ”¥ Puzzles](https://puzzles.modular.com/introduction.html)
- [Triton Puzzles](https://github.com/srush/Triton-Puzzles)

#### Twitter (formerly known as $\mathbb{X}$ ):
- [Elliot Arledge](https://x.com/elliotarledge)
- [maharshi](https://x.com/mrsiipa)
